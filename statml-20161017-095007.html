<html><head>
<script type="text/javascript"
src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {inlineMath: [['$','$'], ['\(','\)']]}
});
</script>
</head><body>
<h1> 2016-10-16T20:30:00-05:00 </h1>
<h2> MML is not consistent for Neyman-Scott. (arXiv:1610.04336v1 [stat.ML]) </h2>
<p>Minimum Message Length (MML) is a popular method for statistical inference,
belonging to the Minimum Description Length (MDL) family. It is a general name
for any of several computationally-feasible approximations to the generally
NP-Hard Strict Minimum Message Length (SMML) estimator. One often-cited
showcase for the power of MML is the Neyman-Scott estimation problem, where
most popular estimation algorithms fail to produce a consistent result. MML's
performance on Neyman-Scott was analysed by Dowe and Wallace (1997) and by
Wallace (2005) and MML was shown to be consistent for the problem. However,
this analysis was not performed on SMML, but rather on two SMML approximations:
Wallace-Freeman and Ideal Group. As for most estimation problems, the exact
SMML solution is not known for Neyman-Scott. We analyse the Dowe-Wallace
solution, and show that it hinges critically on the use of an unnatural prior
for the problem. We argue that the Jeffreys prior is a more natural prior to
assume in this case. Re-analysing the problem over its Jeffreys prior, we show
that both the Ideal Group and the Wallace-Freeman approximations converge to
the (inconsistent) Maximum Likelihood (ML) solution. We develop novel
techniques that enable determining properties of the SMML estimator for some
general families of estimation problems without requiring a full construction
of the estimator, and use these to show that for many problems, including
Neyman-Scott, the SMML estimator is not a point-estimator at all. Rather, it
maps each observation to an entire continuum of estimates. Furthermore, using
the tools developed we show that for Neyman-Scott the SMML estimate is
inconsistent for all parameter sets as well as asymptotically. We discuss
methodological problems in the arguments put forward by previous authors, who
argued that MML is consistent for Neyman-Scott and in general.
</p>
<h2> A Language-independent and Compositional Model for Personality Trait Recognition from Short Texts. (arXiv:1610.04345v1 [cs.CL]) </h2>
<p>Many methods have been used to recognize author personality traits from text,
typically combining linguistic feature engineering with shallow learning
models, e.g. linear regression or Support Vector Machines. This work uses
deep-learning-based models and atomic features of text, the characters, to
build hierarchical, vectorial word and sentence representations for trait
inference. This method, applied to a corpus of tweets, shows state-of-the-art
performance across five traits and three languages (English, Spanish and
Italian) compared with prior work in author profiling. The results, supported
by preliminary visualisation work, are encouraging for the ability to detect
complex human traits.
</p>
<h2> Semi-supervised Graph Embedding Approach to Dynamic Link Prediction. (arXiv:1610.04351v1 [stat.ML]) </h2>
<p>We propose a simple discrete time semi-supervised graph embedding approach to
link prediction in dynamic networks. The learned embedding reflects information
from both the temporal and cross-sectional network structures, which is
performed by defining the loss function as a weighted sum of the supervised
loss from past dynamics and the unsupervised loss of predicting the
neighborhood context in the current network. Our model is also capable of
learning different embeddings for both formation and dissolution dynamics.
These key aspects contributes to the predictive performance of our model and we
provide experiments with three real--world dynamic networks showing that our
method is comparable to state of the art methods in link formation prediction
and outperforms state of the art baseline methods in link dissolution
prediction.
</p>
<h2> Aboveground biomass mapping in French Guiana by combining remote sensing, forest inventories and environmental data. (arXiv:1610.04371v1 [stat.ML]) </h2>
<p>Mapping forest aboveground biomass (AGB) has become an important task,
particularly for the reporting of carbon stocks and changes. AGB can be mapped
using synthetic aperture radar data (SAR) or passive optical data. However,
these data are insensitive to high AGB levels (\textgreater{}150 Mg/ha, and
\textgreater{}300 Mg/ha for P-band), which are commonly found in tropical
forests. Studies have mapped the rough variations in AGB by combining optical
and environmental data at regional and global scales. Nevertheless, these maps
cannot represent local variations in AGB in tropical forests. In this paper, we
hypothesize that the problem of misrepresenting local variations in AGB and AGB
estimation with good precision occurs because of both methodological limits
(signal saturation or dilution bias) and a lack of adequate calibration data in
this range of AGB values. We test this hypothesis by developing a calibrated
regression model to predict variations in high AGB values (mean
\textgreater{}300 Mg/ha) in French Guiana by a methodological approach for
spatial extrapolation with data from the optical geoscience laser altimeter
system (GLAS), forest inventories, radar, optics, and environmental variables
for spatial inter-and extrapolation. Given their higher point count, GLAS data
allow a wider coverage of AGB values. We find that the metrics from GLAS
footprints are correlated with field AGB estimations (R 2 =0.54, RMSE=48.3
Mg/ha) with no bias for high values. First, predictive models, including
remote-sensing, environmental variables and spatial correlation functions,
allow us to obtain "wall-to-wall" AGB maps over French Guiana with an RMSE for
the in situ AGB estimates of ~51 Mg/ha and R${}^2$=0.48 at a 1-km grid size. We
conclude that a calibrated regression model based on GLAS with dependent
environmental data can produce good AGB predictions even for high AGB values if
the calibration data fit the AGB range. We also demonstrate that small temporal
and spatial mismatches between field data and GLAS footprints are not a problem
for regional and global calibrated regression models because field data aim to
predict large and deep tendencies in AGB variations from environmental
gradients and do not aim to represent high but stochastic and temporally
limited variations from forest dynamics. Thus, we advocate including a greater
variety of data, even if less precise and shifted, to better represent high AGB
values in global models and to improve the fitting of these models for high
values.
</p>
<h2> Practical Learning of Deep Gaussian Processes via Random Fourier Features. (arXiv:1610.04386v1 [stat.ML]) </h2>
<p>The composition of multiple Gaussian Processes as a Deep Gaussian Process
(DGP) enables a deep probabilistic approach to flexibly quantify uncertainty
and carry out model selection in various learning scenarios. In this work, we
introduce a novel formulation of DGPs based on random Fourier features that we
train using stochastic variational inference. Our proposal yields an efficient
way of training DGP architectures without compromising on predictive
performance. Through a series of experiments, we illustrate how our model
compares favorably to other state-of-the-art inference methods for DGPs for
both regression and classification tasks. We also demonstrate how an
asynchronous implementation of stochastic gradient optimization can exploit the
computational power of distributed systems for large-scale DGP learning.
</p>
<h2> Theoretical Analysis of Domain Adaptation with Optimal Transport. (arXiv:1610.04420v1 [stat.ML]) </h2>
<p>Domain adaptation (DA) is an important and emerging field of machine learning
that tackles the problem occurring when the distributions of training (source
domain) and test (target domain) data are similar but different. Current
theoretical results show that the efficiency of DA algorithms depends on their
capacity of minimizing the divergence between source and target probability
distributions. In this paper, we provide a theoretical study on the advantages
that concepts borrowed from optimal transportation theory can bring to DA. In
particular, we show that the Wasserstein metric can be used as a divergence
measure between distributions to obtain generalization guarantees for three
different learning settings: (i) classic DA with unsupervised target data (ii)
DA combining source and target labeled data, (iii) multiple source DA. Based on
the obtained results, we provide some insights showing when this analysis can
be tighter than other existing frameworks. We also show that in the context of
multiple source DA, the problem of estimating of the best joint hypothesis
between source and target labeling functions can be reformulated using a
Wasserstein distance-based loss function. We think that these results open the
door to novel ideas and directions for DA.
</p>
<h2> A Reduction Theorem for the Sample Mean in Dynamic Time Warping Spaces. (arXiv:1610.04460v1 [cs.CV]) </h2>
<p>Though the concept of sample mean in dynamic time warping (DTW) spaces is
used in pattern recognition applications, its existence has neither been proved
nor called into question. This article shows that a sample mean exists under
general conditions that cover common variations of different DTW-spaces
mentioned in the literature. The existence proofs are based on a Reduction
Theorem that bounds the length of the candidate solutions we need to consider.
The proposed results place the concept of sample mean in DTW-spaces on a sound
mathematical foundation and serves as a first step towards a statistical theory
of DTW-spaces.
</p>
<h2> Amortised MAP Inference for Image Super-resolution. (arXiv:1610.04490v1 [cs.CV]) </h2>
<p>Image Super-resolution (SR) is an underdetermined inverse problem, where a
large number of plausible high-resolution images can explain the same
downsampled image. Most current single image SR methods use empirical risk
minimisation, often with a pixel-wise mean squared error (MSE) loss. However,
the outputs from such methods tend to be blurry, over-smoothed and generally
appear implausible. A more desirable approach would employ Maximum a Posteriori
(MAP) inference, preferring solutions that always have a high probability under
the image prior, and thus appear more plausible. Direct MAP estimation for SR
is non-trivial, as it requires us to build a model for the image prior from
samples. Furthermore, MAP inference is often performed via optimisation-based
iterative algorithms which don't compare well with the efficiency of
neural-network-based alternatives. Here we introduce new methods for amortised
MAP inference whereby we calculate the MAP estimate directly using a
convolutional neural network. We first introduce a novel neural network
architecture that performs a projection to the affine subspace of valid SR
solutions ensuring that the high resolution output of the network is always
consistent with the low resolution input. We show that, using this
architecture, the amortised MAP inference problem reduces to minimising the
cross-entropy between two distributions, similar to training generative models.
We propose three methods to solve this optimisation problem: (1) Generative
Adversarial Networks (GAN) (2) denoiser-guided SR which backpropagates
gradient-estimates from denoising to train the network, and (3) a baseline
method using a maximum-likelihood-trained image prior. Our experiments show
that the GAN based approach performs best on real image data, achieving
particularly good results in photo-realistic texture SR.
</p>
<h2> The End of Optimism? An Asymptotic Analysis of Finite-Armed Linear Bandits. (arXiv:1610.04491v1 [stat.ML]) </h2>
<p>Stochastic linear bandits are a natural and simple generalisation of
finite-armed bandits with numerous practical applications. Current approaches
focus on generalising existing techniques for finite-armed bandits, notably the
optimism principle and Thompson sampling. While prior work has mostly been in
the worst-case setting, we analyse the asymptotic instance-dependent regret and
show matching upper and lower bounds on what is achievable. Surprisingly, our
results show that no algorithm based on optimism or Thompson sampling will ever
achieve the optimal rate, and indeed, can be arbitrarily far from optimal, even
in very simple cases. This is a disturbing result because these techniques are
standard tools that are widely used for sequential optimisation. For example,
for generalised linear bandits and reinforcement learning.
</p>
<h2> Generalization Error of Invariant Classifiers. (arXiv:1610.04574v1 [stat.ML]) </h2>
<p>This paper studies the generalization error of invariant classifiers. In
particular, we consider the common scenario where the classification task is
invariant to certain transformations of the input, and that the classifier is
constructed (or learned) to be invariant to these transformations. Our approach
relies on factoring the input space into a product of a base space and a set of
transformations. We show that whereas the generalization error of a
non-invariant classifier is proportional to the complexity of the input space,
the generalization error of an invariant classifier is proportional to the
complexity of the base space. We also derive a set of sufficient conditions on
the geometry of the base space and the set of transformations that ensure that
the complexity of the base space is much smaller than the complexity of the
input space. Our analysis applies to general classifiers such as convolutional
neural networks. We demonstrate the implications of the developed theory for
such classifiers with experiments on the MNIST and CIFAR-10 datasets.
</p>
<h2> Improved Strongly Adaptive Online Learning using Coin Betting. (arXiv:1610.04578v1 [stat.ML]) </h2>
<p>This paper describes a new parameter-free online learning algorithm for
changing environments. In comparing against algorithms with the same time
complexity as ours, we obtain a strongly adaptive regret bound that is a factor
of at least $\sqrt{\log(T)}$ better, where $T$ is the time horizon. Empirical
results show that our algorithm outperforms state-of-the-art methods in
learning with expert advice and metric learning scenarios.
</p>
<h2> Message-passing algorithms for synchronization problems over compact groups. (arXiv:1610.04583v1 [cs.IT]) </h2>
<p>Various alignment problems arising in cryo-electron microscopy, community
detection, time synchronization, computer vision, and other fields fall into a
common framework of synchronization problems over compact groups such as Z/L,
U(1), or SO(3). The goal of such problems is to estimate an unknown vector of
group elements given noisy relative observations. We present an efficient
iterative algorithm to solve a large class of these problems, allowing for any
compact group, with measurements on multiple 'frequency channels' (Fourier
modes, or more generally, irreducible representations of the group). Our
algorithm is a highly efficient iterative method following the blueprint of
approximate message passing (AMP), which has recently arisen as a central
technique for inference problems such as structured low-rank estimation and
compressed sensing. We augment the standard ideas of AMP with ideas from
representation theory so that the algorithm can work with distributions over
compact groups. Using standard but non-rigorous methods from statistical
physics we analyze the behavior of our algorithm on a Gaussian noise model,
identifying phases where the problem is easy, (computationally) hard, and
(statistically) impossible. In particular, such evidence predicts that our
algorithm is information-theoretically optimal in many cases, and that the
remaining cases show evidence of statistical-to-computational gaps.
</p>
<h2> Data-Driven Threshold Machine: Scan Statistics, Change-Point Detection, and Extreme Bandits. (arXiv:1610.04599v1 [cs.LG]) </h2>
<p>We present a novel distribution-free approach, the data-driven threshold
machine (DTM), for a fundamental problem at the core of many learning tasks:
choose a threshold for a given pre-specified level that bounds the tail
probability of the maximum of a (possibly dependent but stationary) random
sequence. We do not assume data distribution, but rather relying on the
asymptotic distribution of extremal values, and reduce the problem to estimate
three parameters of the extreme value distributions and the extremal index. We
specially take care of data dependence via estimating extremal index since in
many settings, such as scan statistics, change-point detection, and extreme
bandits, where dependence in the sequence of statistics can be significant. Key
features of our DTM also include robustness and the computational efficiency,
and it only requires one sample path to form a reliable estimate of the
threshold, in contrast to the Monte Carlo sampling approach which requires
drawing a large number of sample paths. We demonstrate the good performance of
DTM via numerical examples in various dependent settings.
</p>
<h2> Second Order Stochastic Optimization in Linear Time. (arXiv:1602.03943v4 [stat.ML] UPDATED) </h2>
<p>First-order stochastic methods are the state-of-the-art in large-scale
machine learning optimization owing to efficient per-iteration complexity.
Second-order methods, while able to provide faster convergence, have been much
less explored due to the high cost of computing the second-order information.
In this paper we develop second-order stochastic methods for optimization
problems in machine learning that match the per-iteration cost of gradient
based methods, and in certain settings improves upon the overall running time
upon the state-of-the-art. Furthermore, our algorithm has the desirable
property of being implementable in time linear in the sparsity of the input
data.
</p>
<h2> Localized Lasso for High-Dimensional Regression. (arXiv:1603.06743v3 [stat.ML] UPDATED) </h2>
<p>We introduce the localized Lasso, which is suited for learning models that
are both interpretable and have a high predictive power in problems with high
dimensionality $d$ and small sample size $n$. More specifically, we consider a
function defined by local sparse models, one at each data point. We introduce
sample-wise network regularization to borrow strength across the models, and
sample-wise exclusive group sparsity (a.k.a., $\ell_{1,2}$ norm) to introduce
diversity into the choice of feature sets in the local models. The local models
are interpretable in terms of similarity of their sparsity patterns. The cost
function is convex, and thus has a globally optimal solution. Moreover, we
propose a simple yet efficient iterative least-squares based optimization
procedure for the localized Lasso, which does not need a tuning parameter, and
is guaranteed to converge to a globally optimal solution. The solution is
empirically shown to outperform alternatives for both simulated and genomic
personalized medicine data.
</p>
<h2> Uncovering Causality from Multivariate Hawkes Integrated Cumulants. (arXiv:1607.06333v2 [stat.ML] UPDATED) </h2>
<p>We design a new nonparametric method that allows one to estimate the matrix
of integrated kernels of a multivariate Hawkes process. This matrix not only
encodes the mutual influences of each nodes of the process, but also
disentangles the causality relationships between them. Our approach is the
first that leads to an estimation of this matrix without any parametric
modeling and estimation of the kernels themselves. A consequence is that it can
give an estimation of causality relationships between nodes (or users), based
on their activity timestamps (on a social network for instance), without
knowing or estimating the shape of the activities lifetime. For that purpose,
we introduce a moment matching method that fits the third-order integrated
cumulants of the process. We show on numerical experiments that our approach is
indeed very robust to the shape of the kernels, and gives appealing results on
the MemeTracker database.
</p>
<h2> Higher-Order Factorization Machines. (arXiv:1607.07195v2 [stat.ML] UPDATED) </h2>
<p>Factorization machines (FMs) are a supervised learning approach that can use
second-order feature combinations even when the data is very high-dimensional.
Unfortunately, despite increasing interest in FMs, there exists to date no
efficient training algorithm for higher-order FMs (HOFMs). In this paper, we
present the first generic yet efficient algorithms for training arbitrary-order
HOFMs. We also present new variants of HOFMs with shared parameters, which
greatly reduce model size and prediction times while maintaining similar
accuracy. We demonstrate the proposed approaches on four different link
prediction tasks.
</p>
<h2> Learning Tree-Structured Detection Cascades for Heterogeneous Networks of Embedded Devices. (arXiv:1608.00159v2 [stat.ML] UPDATED) </h2>
<p>In this paper, we present a new approach to learning cascaded classifiers for
use in computing environments that involve networks of heterogeneous and
resource-constrained, low-power embedded compute and sensing nodes. We present
a generalization of the classical linear detection cascade to the case of
tree-structured cascades where different branches of the tree execute on
different physical compute nodes in the network. Different nodes have access to
different features, as well as access to potentially different computation and
energy resources. We concentrate on the problem of jointly learning the
parameters for all of the classifiers in the cascade given a fixed cascade
architecture and a known set of costs required to carry out the computation at
each node.To accomplish the objective of joint learning of all detectors, we
propose a novel approach to combining classifier outputs during training that
better matches the hard cascade setting in which the learned system will be
deployed. This work is motivated by research in the area of mobile health where
energy efficient real time detectors integrating information from multiple
wireless on-body sensors and a smart phone are needed for real-time monitoring
and delivering just-in-time adaptive interventions. We apply our framework to
the problem of cigarette smoking detection from a combination of wrist-worn
actigraphy data and respiration chest band data.
</p>
<h2> Stochastic Rank-1 Bandits. (arXiv:1608.03023v2 [cs.LG] UPDATED) </h2>
<p>We propose stochastic rank-$1$ bandits, a class of online learning problems
where at each step a learning agent chooses a pair of row and column arms, and
receives the product of their values as a reward. The challenge is that the
values of the row and column are unobserved. These values are stochastic and
drawn independently of each other. We propose an efficient algorithm for
solving our problem, Rank1Elim, and derive a $O((K + L) (1 / \Delta) \log n)$
upper bound on its $n$-step regret, where $K$ is the number of rows, $L$ is the
number of columns, and $\Delta$ is the minimum of the row and column gaps. This
is the first bandit algorithm for finding the maximum entry of a rank-$1$
matrix whose regret is linear in $K + L$, $1 / \Delta$, and $\log n$. We
evaluate our proposed algorithm on both synthetic and real-world problems, and
observe that it leverages the structure of our problems and can learn
near-optimal solutions even when our modeling assumptions are mildly violated.
</p>
<h2> Understanding intermediate layers using linear classifier probes. (arXiv:1610.01644v3 [stat.ML] UPDATED) </h2>
<p>Neural network models have a reputation for being black boxes. We propose a
new method to understand better the roles and dynamics of the intermediate
layers. This has direct consequences on the design of such models and it
enables the expert to be able to justify certain heuristics (such as the
auxiliary heads in the Inception model). Our method uses linear classifiers,
referred to as "probes", where a probe can only use the hidden units of a given
intermediate layer as discriminating features. Moreover, these probes cannot
affect the training phase of a model, and they are generally added after
training. They allow the user to visualize the state of the model at multiple
steps of training. We demonstrate how this can be used to develop a better
intuition about a known model and to diagnose potential problems.
</p>
<h2> The Generalized Reparameterization Gradient. (arXiv:1610.02287v2 [stat.ML] UPDATED) </h2>
<p>The reparameterization gradient has become a widely used method to obtain
Monte Carlo gradients to optimize the variational objective. However, this
technique does not easily apply to commonly used distributions such as beta or
gamma without further approximations, and most practical applications of the
reparameterization gradient fit Gaussian distributions. In this paper, we
introduce the generalized reparameterization gradient, a method that extends
the reparameterization gradient to a wider class of variational distributions.
Generalized reparameterizations use invertible transformations of the latent
variables which lead to transformed distributions that weakly depend on the
variational parameters. This results in new Monte Carlo gradients that combine
reparameterization gradients and score function gradients. We demonstrate our
approach on variational inference for two complex probabilistic models. The
generalized reparameterization is effective: even a single sample from the
variational distribution is enough to obtain a low-variance gradient.
</p>
<h2> A nonparametric sequential test for online randomized experiments. (arXiv:1610.02490v2 [stat.ML] UPDATED) </h2>
<p>We propose a nonparametric sequential test that aims to address two practical
problems pertinent to online randomized experiments: (i) how to do a hypothesis
test for complex metrics; (ii) how to prevent type $1$ error inflation under
continuous monitoring. The proposed test does not require knowledge of the
underlying probability distribution generating the data. We use the bootstrap
to estimate the likelihood for blocks of data followed by mixture sequential
probability ratio test. We validate this procedure on data from a major online
e-commerce website and show that the proposed test controls type $1$ error at
any time, has good power, and allows quick inference in online randomized
experiments.
</p>
<h2> Post Selection Inference with Kernels. (arXiv:1610.03725v2 [stat.ML] UPDATED) </h2>
<p>We propose a novel kernel based post selection inference (PSI) algorithm,
which can not only handle non-linearity in data but also structured output such
as multi-dimensional and multi-label outputs. Specifically, we develop a PSI
algorithm for independence measures, and propose the Hilbert-Schmidt
Independence Criterion (HSIC) based PSI algorithm (hsicInf). The novelty of the
proposed algorithm is that it can handle non-linearity and/or structured data
through kernels. Namely, the proposed algorithm can be used for wider range of
applications including nonlinear multi-class classification and multi-variate
regressions, while existing PSI algorithms cannot handle them. Through
synthetic experiments, we show that the proposed approach can find a set of
statistically significant features for both regression and classification
problems. Moreover, we apply the hsicInf algorithm to a real-world data, and
show that hsicInf can successfully identify important features.
</p>
<h2> Semi-Supervised Active Learning for Support Vector Machines: A Novel Approach that Exploits Structure Information in Data. (arXiv:1610.03995v2 [stat.ML] UPDATED) </h2>
<p>In our today's information society more and more data emerges, e.g.~in social
networks, technical applications, or business applications. Companies try to
commercialize these data using data mining or machine learning methods. For
this purpose, the data are categorized or classified, but often at high
(monetary or temporal) costs. An effective approach to reduce these costs is to
apply any kind of active learning (AL) methods, as AL controls the training
process of a classifier by specific querying individual data points (samples),
which are then labeled (e.g., provided with class memberships) by a domain
expert. However, an analysis of current AL research shows that AL still has
some shortcomings. In particular, the structure information given by the
spatial pattern of the (un)labeled data in the input space of a classification
model (e.g.,~cluster information), is used in an insufficient way. In addition,
many existing AL techniques pay too little attention to their practical
applicability. To meet these challenges, this article presents several
techniques that together build a new approach for combining AL and
semi-supervised learning (SSL) for support vector machines (SVM) in
classification tasks. Structure information is captured by means of
probabilistic models that are iteratively improved at runtime when label
information becomes available. The probabilistic models are considered in a
selection strategy based on distance, density, diversity, and distribution (4DS
strategy) information for AL and in a kernel function (Responsibility Weighted
Mahalanobis kernel) for SVM. The approach fuses generative and discriminative
modeling techniques. With 20 benchmark data sets and with the MNIST data set it
is shown that our new solution yields significantly better results than
state-of-the-art methods.
</p>
<h2> Gated End-to-End Memory Networks. (arXiv:1610.04211v1 [cs.CL] CROSS LISTED) </h2>
<p>Machine reading using differentiable reasoning models has recently shown
remarkable progress. In this context, End-to-End trainable Memory Networks,
MemN2N, have demonstrated promising performance on simple natural language
based reasoning tasks such as factual reasoning and basic deduction. However,
other tasks, namely multi-fact question-answering, positional reasoning or
dialog related tasks, remain challenging particularly due to the necessity of
more complex interactions between the memory and controller modules composing
this family of models. In this paper, we introduce a novel end-to-end memory
access regulation mechanism inspired by the current progress on the connection
short-cutting principle in the field of computer vision. Concretely, we develop
a Gated End-to-End trainable Memory Network architecture, GMemN2N. From the
machine learning perspective, this new capability is learned in an end-to-end
fashion without the use of any additional supervision signal which is, as far
as our knowledge goes, the first of its kind. Our experiments show significant
improvements on the most challenging tasks in the 20 bAbI dataset, without the
use of any domain knowledge. Then, we show improvements on the dialog bAbI
tasks including the real human-bot conversion-based Dialog State Tracking
Challenge (DSTC-2) dataset. On these two datasets, our model sets the new state
of the art.
</p>
</body></html>
